current_column[3] %>% flatten %>% map(function(row_list) {
cur_hist_bar = (row_list %>% flatten)
bar_name = cur_hist_bar[1]
bar_counts = cur_hist_bar[2]
names(cur_hist_bar)[1] = paste0(column_name)
names(cur_hist_bar)[2] = "Counts"
cur_hist_bar
})
}
prepped = barValues %>% map(flatten) %>% bind_rows()
require(scales)
target_order = unlist((prepped[,1]), use.names=FALSE)
Counts='Counts'
plot = if (include_null == TRUE){
ggplot(data=prepped,
aes(x=!!ensym(column_name),
y=!!ensym(Counts),
fill=factor({ifelse(!!ensym(column_name)=="nulls","Highlighted","Normal")}),
col=factor({ifelse(!!ensym(column_name)=="nulls","Highlighted","Normal")})
)
)+
geom_bar(stat="identity", width=0.75, alpha=.2, show.legend=FALSE) +
scale_fill_manual(name = paste0(column_name), values=c("red","green")) +
scale_color_manual(name = paste0(column_name), values=c("darkred","darkgreen")) +
scale_y_continuous(labels=comma, breaks = scales::pretty_breaks(n = 6)) +
scale_x_discrete(limits=target_order) +
theme(axis.text.x = element_text(angle=60, hjust=1, vjust=0.5),
axis.title.x = element_text(margin = margin(t = 20, r = 0, b = 0, l = 0), size=15),
axis.title.y = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), size=15)
)
}else{
ggplot(data=prepped,
aes_string(x=paste0(column_name),
y='Counts')
) +
geom_bar(stat="identity", width=0.75, col="darkgreen", fill="green", alpha=.2) +
scale_y_continuous(labels=comma, breaks = scales::pretty_breaks(n = 6)) +
scale_x_discrete(limits=target_order) +
theme(axis.text.x = element_text(angle=60, hjust=1, vjust=0.5),
axis.title.x = element_text(margin = margin(t = 20, r = 0, b = 0, l = 0), size=15),
axis.title.y = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0), size=15)
)
}
if (print_plot == TRUE){
print(plot)
}else {
outObject = list(column_name, plot)
names(outObject[1]) = "chartName"
names(outObject[2]) = "plot"
outObject
}
})
plots
}
spark2.describe_extra = function(sparklyr_table, round_at=2L){
summed = sdf_describe(sparklyr_table) %>% sdf_collect()
collected = sparklyr::invoke_new(sc, 'com.gabechurch.sparklyRWrapper') %>%
sparklyr::invoke('getDistinctCounts', spark_dataframe(sparklyr_table), FALSE) %>%
sdf_collect()
column_names = colnames(collected)
descriptions = column_names %>% map(function(column_name){
#Flatten
testout = summed %>% select(summary, paste0(column_name))
df = add_rownames(testout) %>% gather(var, value, -rowname) %>% spread(rowname, value)
newDF = df %>% filter(var != "summary")
name_vals = df %>% filter(var == "summary") %>% unlist %>% unname
colnames(newDF) = name_vals
#colnames(df) = as.character(unlist(df[1,]))
#df = df[-1,]
#Combine with Flattened
summary = as.data.frame(collected) %>% select(paste0(column_name))
newDF["count_distinct"] = summary[,paste0(column_name)]
newDF[c('summary','count','count_distinct','mean','stddev','min','max')]
}) %>% rbind_all
descriptions$mean = round(as.numeric(descriptions$mean), round_at)
descriptions$stddev = round(as.numeric(descriptions$stddev), round_at)
descriptions$max = round(as.numeric(descriptions$max), round_at)
descriptions$min = round(as.numeric(descriptions$min), round_at)
descriptions
}
#
grpd_ordrd_descriptions = grouped_plots %>% map(function(plotGroup){
plotAName = plotGroup[[1]][[1]]
plotADescOrig = extra_described %>% filter(summary == paste0(plotAName))
plotADescription = plotADescOrig %>%
datatable(plotADescOrig, rownames = FALSE, options = list(
autoWidth = TRUE,
initComplete = JS(
"function(settings, json) {",
"$(this.api().table().header()).css({'font-size': '12px', 'background-color': '#66bd3c', 'color': '#fff'});",
"}"))) %>%  formatStyle(columns = colnames(plotADescOrig), 'font-size' = '12px')
outputGroup = if(length(plotGroup) == 1){
list(plotADescription)
}else{
plotBName = plotGroup[[2]][[1]]
plotBDescOrig = extra_described %>% filter(summary == paste0(plotBName))
plotBDescription = datatable(plotBDescOrig, rownames=FALSE, options = list(
columnDefs = list(list(className='dt-center', targets = "_all")),
autoWidth = TRUE,
initComplete = JS(
"function(settings, json) {",
"$(this.api().table().header()).css({'font-size':'12px','background-color': '#66bd3c','color':'#fff'});",
"}")
)) %>%  formatStyle(columns = colnames(plotBDescOrig), 'font-size' = '12px')
list(plotADescription, plotBDescription)
}
})
grpd_ordrd_descriptions[[1]][[1]]
#
grpd_ordrd_descriptions = grouped_plots %>% map(function(plotGroup){
plotAName = plotGroup[[1]][[1]]
plotADescOrig = extra_described %>% filter(summary == paste0(plotAName))
plotADescription = plotADescOrig %>%
datatable(plotADescOrig, rownames = FALSE, options = list(
columnDefs = list(list(className='dt-center', targets = "_all")),
autoWidth = TRUE,
initComplete = JS(
"function(settings, json) {",
"$(this.api().table().header()).css({'font-size': '12px', 'background-color': '#66bd3c', 'color': '#fff'});",
"}"))) %>%  formatStyle(columns = colnames(plotADescOrig), 'font-size' = '12px')
outputGroup = if(length(plotGroup) == 1){
list(plotADescription)
}else{
plotBName = plotGroup[[2]][[1]]
plotBDescOrig = extra_described %>% filter(summary == paste0(plotBName))
plotBDescription = datatable(plotBDescOrig, rownames=FALSE, options = list(
columnDefs = list(list(className='dt-center', targets = "_all")),
autoWidth = TRUE,
initComplete = JS(
"function(settings, json) {",
"$(this.api().table().header()).css({'font-size':'12px','background-color':'#66bd3c','color':'#fff'});",
"}")
)) %>%  formatStyle(columns = colnames(plotBDescOrig), 'font-size' = '12px')
list(plotADescription, plotBDescription)
}
})
grpd_ordrd_descriptions[[2]][[2]]
# and split here
grpd_ordrd_descriptions[[1]][[1]]
#
grpd_ordrd_descriptions = grouped_plots %>% map(function(plotGroup){
plotAName = plotGroup[[1]][[1]]
plotADescOrig = extra_described %>% filter(summary == paste0(plotAName))
plotADescription = plotADescOrig %>%
datatable(plotADescOrig, rownames = FALSE, options = list(
columnDefs = list(list(className='dt-center', targets = "_all")),
autoWidth = TRUE,
initComplete = JS(
"function(settings, json) {",
"$(this.api().table().header()).css({'font-size': '16px', 'background-color': '#66bd3c', 'color': '#fff'});",
"}"))) %>%  formatStyle(columns = colnames(plotADescOrig), 'font-size' = '12px')
outputGroup = if(length(plotGroup) == 1){
list(plotADescription)
}else{
plotBName = plotGroup[[2]][[1]]
plotBDescOrig = extra_described %>% filter(summary == paste0(plotBName))
plotBDescription = datatable(plotBDescOrig, rownames=FALSE, options = list(
columnDefs = list(list(className='dt-center', targets = "_all")),
autoWidth = TRUE,
initComplete = JS(
"function(settings, json) {",
"$(this.api().table().header()).css({'font-size':'16px','background-color':'#66bd3c','color':'#fff'});",
"}")
)) %>%  formatStyle(columns = colnames(plotBDescOrig), 'font-size' = '12px')
list(plotADescription, plotBDescription)
}
})
grpd_ordrd_descriptions[[2]][[2]]
# and split here
library(sparklyr)
library(sparkedatools)
library(dplyr)
Sys.setenv(SPARK_HOME="/usr/hdp/3.1.0.0-78/spark2")
conf = spark_config()
conf$'sparklyr.jars.default'= "/home/gchurch/R/sparkeda_2.11-2.11.jar"
conf$'sparklyr.shell.executor-memory' = "4g"
conf$'sparklyr.shell.driver-memory' = "70g"
conf$spark.io.compression = "Snappy" #serialization
conf$spark.serializer="org.apache.spark.serializer.KryoSerializer"
conf$spark.kryo.unsafe="true"
conf$spark.shuffle.service.enabled="true"
conf$spark.shuffle.compress="true"
conf$spark.shuffle.consolidateFiles="true" #adding dynamic allocation
conf$spark.dynamicAllocation.enabled = "true"
conf$spark.dynamicAllocation.initialExecutors=0
conf$spark.dynamicAllocation.maxExecutors=30
conf$spark.dynamicAllocation.minExecutors=0
#other
conf$spark.default.parallelism=10
sc = spark_connect(master = "yarn-client", config = conf, version = '2.3.2')
tbl_cache(sc, "sample_data.cc_fraud")
cc_fraud = tbl(sc, tbl("sample_data.cc_fraud"))
library(sparklyr)
library(sparkedatools)
library(dplyr)
Sys.setenv(SPARK_HOME="/usr/hdp/3.1.0.0-78/spark2")
conf = spark_config()
conf$'sparklyr.jars.default'= "/home/gchurch/R/sparkeda_2.11-2.11.jar"
conf$'sparklyr.shell.executor-memory' = "4g"
conf$'sparklyr.shell.driver-memory' = "70g"
conf$spark.io.compression = "Snappy" #serialization
conf$spark.serializer="org.apache.spark.serializer.KryoSerializer"
conf$spark.kryo.unsafe="true"
conf$spark.shuffle.service.enabled="true"
conf$spark.shuffle.compress="true"
conf$spark.shuffle.consolidateFiles="true" #adding dynamic allocation
conf$spark.dynamicAllocation.enabled = "true"
conf$spark.dynamicAllocation.initialExecutors=0
conf$spark.dynamicAllocation.maxExecutors=30
conf$spark.dynamicAllocation.minExecutors=0
#other
conf$spark.default.parallelism=10
sc = spark_connect(master = "yarn-client", config = conf, version = '2.3.2')
tbl_cache(sc, "sample_data.cc_fraud")
cc_fraud = tbl(sc, sql("select * from sample_data.cc_fraud"))
final_output = spark_eda_dash(cc_fraud,
hist_num_buckets = 10L,
hist_include_null = TRUE,
hist_decimal_places = 2L,
desc_decimal_places = 2L
)
knitr::opts_chunk$set(echo = TRUE)
spark_disconnect_all()
library(sparklyr)
library(sparkedatools)
library(dplyr)
Sys.setenv(SPARK_HOME="/usr/hdp/3.1.0.0-78/spark2")
conf = spark_config()
conf$'sparklyr.jars.default'= "/home/gchurch/R/sparkeda_2.11-2.11.jar"
conf$'sparklyr.shell.executor-memory' = "4g"
conf$'sparklyr.shell.driver-memory' = "70g"
conf$spark.io.compression = "Snappy" #serialization
conf$spark.serializer="org.apache.spark.serializer.KryoSerializer"
conf$spark.kryo.unsafe="true"
conf$spark.shuffle.service.enabled="true"
conf$spark.shuffle.compress="true"
conf$spark.shuffle.consolidateFiles="true" #adding dynamic allocation
conf$spark.dynamicAllocation.enabled = "true"
conf$spark.dynamicAllocation.initialExecutors=0
conf$spark.dynamicAllocation.maxExecutors=30
conf$spark.dynamicAllocation.minExecutors=0
#other
conf$spark.default.parallelism=10
sc = spark_connect(master = "yarn-client", config = conf, version = '2.3.2')
library(sparklyr)
library(sparkedatools)
library(dplyr)
Sys.setenv(SPARK_HOME="/usr/hdp/3.1.0.0-78/spark2")
conf = spark_config()
conf$'sparklyr.jars.default'= "/home/gchurch/R/sparkeda_2.11-2.11.jar"
conf$'sparklyr.shell.executor-memory' = "4g"
conf$'sparklyr.shell.driver-memory' = "70g"
conf$spark.io.compression = "Snappy" #serialization
conf$spark.serializer="org.apache.spark.serializer.KryoSerializer"
conf$spark.kryo.unsafe="true"
conf$spark.shuffle.service.enabled="true"
conf$spark.shuffle.compress="true"
conf$spark.shuffle.consolidateFiles="true" #adding dynamic allocation
conf$spark.dynamicAllocation.enabled = "true"
conf$spark.dynamicAllocation.initialExecutors=0
conf$spark.dynamicAllocation.maxExecutors=30
conf$spark.dynamicAllocation.minExecutors=0
#other
conf$spark.default.parallelism=10
sc = spark_connect(master = "yarn-client", config = conf, version = '2.3.2')
tbl_cache(sc, "sample_data.cc_fraud")
cc_fraud = tbl(sc, sql("select * from sample_data.cc_fraud"))
final_output = spark_eda_dash(cc_fraud,
hist_num_buckets = 10L,
hist_include_null = TRUE,
hist_decimal_places = 2L,
desc_decimal_places = 2L
)
library(sparklyr)
library(sparkedatools)
library(dplyr)
Sys.setenv(SPARK_HOME="/usr/hdp/3.1.0.0-78/spark2")
conf = spark_config()
conf$'sparklyr.jars.default'= "/home/gchurch/R/sparkeda_2.11-2.11.jar"
conf$'sparklyr.shell.executor-memory' = "4g"
conf$'sparklyr.shell.driver-memory' = "70g"
conf$spark.io.compression = "Snappy" #serialization
conf$spark.serializer="org.apache.spark.serializer.KryoSerializer"
conf$spark.kryo.unsafe="true"
conf$spark.shuffle.service.enabled="true"
conf$spark.shuffle.compress="true"
conf$spark.shuffle.consolidateFiles="true" #adding dynamic allocation
conf$spark.dynamicAllocation.enabled = "true"
conf$spark.dynamicAllocation.initialExecutors=0
conf$spark.dynamicAllocation.maxExecutors=30
conf$spark.dynamicAllocation.minExecutors=0
#other
conf$spark.default.parallelism=10
sc = spark_connect(master = "yarn-client", config = conf, version = '2.3.2')
tbl_cache(sc, "sample_data.cc_fraud")
cc_fraud = tbl(sc, sql("select * from sample_data.cc_fraud"))
final_output = spark_eda_dash(cc_fraud,
hist_num_buckets = 10L,
hist_include_null = TRUE,
hist_decimal_places = 2L,
desc_decimal_places = 2L
)
spark_describe_ext = function(sparklyr_table, round_at=2L){
library(sparklyr)
library(dplyr)
library(purrr)
library(lazyeval)
summed = sdf_describe(sparklyr_table) %>% sdf_collect()
collected = sparklyr::invoke_new(sc, 'com.gabechurch.sparklyRWrapper') %>%
sparklyr::invoke('getDistinctCounts', spark_dataframe(sparklyr_table), FALSE) %>%
sdf_collect()
column_names = colnames(collected)
descriptions = column_names %>% map(function(column_name){
#Flatten
testout = summed %>% select(summary, paste0(column_name))
df = add_rownames(testout) %>% gather(var, value, -rowname) %>% spread(rowname, value)
newDF = df %>% filter(var != "summary")
name_vals = df %>% filter(var == "summary") %>% unlist %>% unname
colnames(newDF) = name_vals
#colnames(df) = as.character(unlist(df[1,]))
#df = df[-1,]
#Combine with Flattened
summary = as.data.frame(collected) %>% select(paste0(column_name))
newDF["count_distinct"] = summary[,paste0(column_name)]
newDF[c('summary','count','count_distinct','mean','stddev','min','max')]
}) %>% rbind_all
descriptions$mean = round(as.numeric(descriptions$mean), round_at)
descriptions$stddev = round(as.numeric(descriptions$stddev), round_at)
descriptions$max = round(as.numeric(descriptions$max), round_at)
descriptions$min = round(as.numeric(descriptions$min), round_at)
descriptions
}
#' A Histogram Function for SparklyR \cr
#' @description
#' This function  is especially useful for EDA on large Spark/Hive tables, it is designed to resemble the hist() function in native R. It should be noted that this implementation does differ from native R, and will "bucket" the data-points. \cr
#' All computation is efficient and distributed in native scala/Spark \cr
#' \cr
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(sparklyr)
library(dplyr)
library(plotly)
library(sparkedatools)
Sys.setenv(SPARK_HOME="/usr/hdp/3.1.0.0-78/spark2")
conf = spark_config()
conf$'sparklyr.jars.default'= "/home/gchurch/R/sparkeda_2.11-2.09.jar"
conf$'sparklyr.shell.executor-memory' = "4g"
conf$'sparklyr.shell.driver-memory' = "70g"
conf$spark.io.compression = "Snappy" #serialization
conf$spark.serializer="org.apache.spark.serializer.KryoSerializer"
conf$spark.kryo.unsafe="true"
conf$spark.shuffle.service.enabled="true"
conf$spark.shuffle.compress="true"
conf$spark.shuffle.consolidateFiles="true" #adding dynamic allocation
conf$spark.dynamicAllocation.enabled = "true"
conf$spark.dynamicAllocation.initialExecutors=0
conf$spark.dynamicAllocation.maxExecutors=30
conf$spark.dynamicAllocation.minExecutors=0
#other
conf$spark.default.parallelism=10
sc = spark_connect(master = "yarn-client", config = conf, version = '2.3.2')
cc_fraud = tbl(sc, sql("select * from sample_data.cc_fraud"))
testout = sdf_describe(cc_fraud)
#summary(cars)
df = add_rownames(testout %>% collect) %>% gather(var, value, -rowname) %>% spread(rowname, value)
colnames(df) = as.character(unlist(df[1,]))
df = df[-1,]
plot_ly(type="table",header=list(values=names(df)), cells=list(values=unname(df)))
library(sparklyr)
library(sparkedatools)
library(dplyr)
Sys.setenv(SPARK_HOME="/usr/hdp/3.1.0.0-78/spark2")
conf = spark_config()
conf$'sparklyr.jars.default'= "/home/gchurch/R/sparkeda_2.11-2.11.jar"
conf$'sparklyr.shell.executor-memory' = "4g"
conf$'sparklyr.shell.driver-memory' = "70g"
conf$spark.io.compression = "Snappy" #serialization
conf$spark.serializer="org.apache.spark.serializer.KryoSerializer"
conf$spark.kryo.unsafe="true"
conf$spark.shuffle.service.enabled="true"
conf$spark.shuffle.compress="true"
conf$spark.shuffle.consolidateFiles="true" #adding dynamic allocation
conf$spark.dynamicAllocation.enabled = "true"
conf$spark.dynamicAllocation.initialExecutors=0
conf$spark.dynamicAllocation.maxExecutors=30
conf$spark.dynamicAllocation.minExecutors=0
#other
conf$spark.default.parallelism=10
sc = spark_connect(master = "yarn-client", config = conf, version = '2.3.2')
tbl_cache(sc, "sample_data.cc_fraud")
cc_fraud = tbl(sc, sql("select * from sample_data.cc_fraud"))
final_output = spark_eda_dash(cc_fraud,
hist_num_buckets = 10L,
hist_include_null = TRUE,
hist_decimal_places = 2L,
desc_decimal_places = 2L
)
knitr::opts_chunk$set(echo = TRUE)
spark_disconnect_all()
library(sparklyr)
library(sparkedatools)
library(dplyr)
Sys.setenv(SPARK_HOME="/usr/hdp/3.1.0.0-78/spark2")
conf = spark_config()
conf$'sparklyr.jars.default'= "/home/gchurch/R/sparkeda_2.11-2.11.jar"
conf$'sparklyr.shell.executor-memory' = "4g"
conf$'sparklyr.shell.driver-memory' = "70g"
conf$spark.io.compression = "Snappy" #serialization
conf$spark.serializer="org.apache.spark.serializer.KryoSerializer"
conf$spark.kryo.unsafe="true"
conf$spark.shuffle.service.enabled="true"
conf$spark.shuffle.compress="true"
conf$spark.shuffle.consolidateFiles="true" #adding dynamic allocation
conf$spark.dynamicAllocation.enabled = "true"
conf$spark.dynamicAllocation.initialExecutors=0
conf$spark.dynamicAllocation.maxExecutors=30
conf$spark.dynamicAllocation.minExecutors=0
#other
conf$spark.default.parallelism=10
sc = spark_connect(master = "yarn-client", config = conf, version = '2.3.2')
library(sparklyr)
library(sparkedatools)
library(dplyr)
Sys.setenv(SPARK_HOME="/usr/hdp/3.1.0.0-78/spark2")
conf = spark_config()
conf$'sparklyr.jars.default'= "/home/gchurch/R/sparkeda_2.11-2.11.jar"
conf$'sparklyr.shell.executor-memory' = "4g"
conf$'sparklyr.shell.driver-memory' = "70g"
conf$spark.io.compression = "Snappy" #serialization
conf$spark.serializer="org.apache.spark.serializer.KryoSerializer"
conf$spark.kryo.unsafe="true"
conf$spark.shuffle.service.enabled="true"
conf$spark.shuffle.compress="true"
conf$spark.shuffle.consolidateFiles="true" #adding dynamic allocation
conf$spark.dynamicAllocation.enabled = "true"
conf$spark.dynamicAllocation.initialExecutors=0
conf$spark.dynamicAllocation.maxExecutors=30
conf$spark.dynamicAllocation.minExecutors=0
#other
conf$spark.default.parallelism=10
sc = spark_connect(master = "yarn-client", config = conf, version = '2.3.2')
tbl_cache(sc, "sample_data.cc_fraud")
cc_fraud = tbl(sc, sql("select * from sample_data.cc_fraud"))
final_output = sparkedatools:::spark_eda_dash(cc_fraud,
hist_num_buckets = 10L,
hist_include_null = TRUE,
hist_decimal_places = 2L,
desc_decimal_places = 2L
)
knitr::opts_chunk$set(echo = TRUE)
spark_disconnect_all()
library(sparklyr)
library(sparkedatools)
library(dplyr)
Sys.setenv(SPARK_HOME="/usr/hdp/3.1.0.0-78/spark2")
conf = spark_config()
conf$'sparklyr.jars.default'= "/home/gchurch/R/sparkeda_2.11-2.11.jar"
conf$'sparklyr.shell.executor-memory' = "4g"
conf$'sparklyr.shell.driver-memory' = "70g"
conf$spark.io.compression = "Snappy" #serialization
conf$spark.serializer="org.apache.spark.serializer.KryoSerializer"
conf$spark.kryo.unsafe="true"
conf$spark.shuffle.service.enabled="true"
conf$spark.shuffle.compress="true"
conf$spark.shuffle.consolidateFiles="true" #adding dynamic allocation
conf$spark.dynamicAllocation.enabled = "true"
conf$spark.dynamicAllocation.initialExecutors=0
conf$spark.dynamicAllocation.maxExecutors=30
conf$spark.dynamicAllocation.minExecutors=0
#other
conf$spark.default.parallelism=10
sc = spark_connect(master = "yarn-client", config = conf, version = '2.3.2')
tbl_cache(sc, "sample_data.cc_fraud")
cc_fraud = tbl(sc, sql("select * from sample_data.cc_fraud"))
final_output = sparkedatools:::spark_eda_dash(cc_fraud,
hist_num_buckets = 10L,
hist_include_null = TRUE,
hist_decimal_places = 2L,
desc_decimal_places = 2L
)
tbl_cache(sc, "sample_data.cc_fraud")
cc_fraud = tbl(sc, sql("select * from sample_data.cc_fraud"))
?spark_describe_ext
?spark_describe_ext
?spark_describe_ext
library(sparkedatools)
?spark_describe_ext
library(sparkedatools)
?sparkedatools:::spark_describe_ext
library(sparkedatools)
?sparkedatools:::spark_describe_ext
library(sparkedatools)
?sparkedatools:::spark_describe_ext
library(sparkedatools)
?sparkedatools:::spark_describe_ext
library(sparkedatools)
?sparkedatools:::spark_describe_ext
library(sparkedatools)
?sparkedatools:::spark_describe_ext
library(sparkedatools)
?sparkedatools:::spark_describe_ext
library(sparkedatools)setwd("/home/gchurch/R/packages/sparkedatools")
setwd("/home/gchurch/R/packages/sparkedatools")
setwd("/home/gchurch/R/packages/")
devtools::uninstall("sparkedatools")
devtools::uninstall("sparkedatools")
setwd("/home/gchurch/R/packages/sparkedatools")
devtools::document()
q()
devtools::document()
devtools::document()
q()
