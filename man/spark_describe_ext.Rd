% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/spark_describe_ext.R
\name{spark_describe_ext}
\alias{spark_describe_ext}
\title{A more descriptive version of spark describe including distinct counts \cr}
\usage{
spark_describe_ext(sparklyr_table, round_at = 2L)
}
\arguments{
\item{sparklyr_table}{is the spark table you will pass to the function. You can pass using a dplyr spark table (tbl).}

\item{round_at}{(default = 2L) controls the number of decimals values to round output counts to (for long outputs)}
}
\description{
This function  is especially useful for EDA on large Spark/Hive tables, it is designed to resemble the hist() function in native R. It should be noted that this implementation does differ from native R, and will "bucket" the data-points. \cr
All computation is efficient and distributed in native scala/Spark \cr
\cr
It is adivsed to drop time/array/other columns (or those with nested datatypes) before running.
}
\details{
Important package requirements: \cr
Download the required jar at \href{www.gabechurch.com/sparkEDA}{www.gabechurch.com/sparkEDA}  (default future integration is in the works) \cr
\cr
Example selection of a spark table and graph\cr
\code{spark_table = tbl(sc, sql("select * from db.stock_samples_20m limit 100"))} \cr
\code{spark_hist(spark_table, 20L)}
}
